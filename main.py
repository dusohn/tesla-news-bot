-import os
import sys
import re
import json
import time
import datetime
from typing import Dict, List, Any, Optional, Tuple
from urllib.parse import quote

import requests
import pytz
from bs4 import BeautifulSoup

# -------------------------------
# Environment Variables
# -------------------------------
OPENAI_API_KEY = (os.environ.get("OPENAI_API_KEY") or "").strip()
TELEGRAM_TOKEN = (os.environ.get("TELEGRAM_TOKEN") or "").strip()
CHAT_ID = (os.environ.get("CHAT_ID") or "").strip()

OPENAI_MODEL = (os.environ.get("OPENAI_MODEL") or "gpt-4o-mini").strip()
OPENAI_URL = "https://api.openai.com/v1/responses"

# Telegram message hard limit is 4096
TELEGRAM_CHUNK_SIZE = 3800

# Finviz anti-bot softening
FINVIZ_SLEEP_SEC = 1.0

MAG7 = [
    {"name": "Apple", "ticker": "AAPL", "emoji": "ğŸ"},
    {"name": "Microsoft", "ticker": "MSFT", "emoji": "ğŸ’»"},
    {"name": "Amazon", "ticker": "AMZN", "emoji": "ğŸ“¦"},
    {"name": "Alphabet", "ticker": "GOOGL", "emoji": "ğŸ”"},
    {"name": "Meta", "ticker": "META", "emoji": "ğŸ§ "},
    {"name": "NVIDIA", "ticker": "NVDA", "emoji": "ğŸ¤–"},
    {"name": "Tesla", "ticker": "TSLA", "emoji": "ğŸš—"},
]


# -------------------------------
# OpenAI Responses API helpers
# -------------------------------
def _extract_output_text(res_json: dict) -> str:
    """
    Responses API ì‘ë‹µì—ì„œ output_textë§Œ í•©ì³ ì¶”ì¶œ
    """
    text_parts = []
    for item in (res_json.get("output") or []):
        for c in (item.get("content") or []):
            if c.get("type") == "output_text" and isinstance(c.get("text"), str):
                text_parts.append(c["text"])
    return "\n".join(t.strip() for t in text_parts if t and t.strip()).strip()


# -------------------------------
# Finviz time parsing (ET -> KST filtering)
# -------------------------------
def _parse_finviz_dt_et(raw: str, now_et: datetime.datetime, last_date_et: Optional[datetime.date]) -> Optional[datetime.datetime]:
    """
    Finviz ë‰´ìŠ¤ í…Œì´ë¸”ì˜ ì‹œê°„ ë¬¸ìì—´ì„ US/Eastern aware datetimeìœ¼ë¡œ íŒŒì‹±.
    ì§€ì› ì˜ˆ:
      - "Feb-03-26 08:35AM"
      - "Today 08:35AM"
      - "08:12AM" (ì´ ê²½ìš° last_date_et í•„ìš”)
    """
    et = pytz.timezone("US/Eastern")
    s = (raw or "").strip()
    if not s:
        return None

    # Today 08:35AM
    if s.lower().startswith("today"):
        parts = s.split()
        if len(parts) >= 2:
            tstr = parts[-1]
            try:
                t = datetime.datetime.strptime(tstr, "%I:%M%p").time()
                return et.localize(datetime.datetime(now_et.year, now_et.month, now_et.day, t.hour, t.minute))
            except Exception:
                return None
        return None

    # "Feb-03-26 08:35AM"
    try:
        dt = datetime.datetime.strptime(s, "%b-%d-%y %I:%M%p")
        return et.localize(dt)
    except Exception:
        pass

    # "08:12AM" (time only)
    try:
        t = datetime.datetime.strptime(s, "%I:%M%p").time()
        if last_date_et is None:
            return None
        return et.localize(datetime.datetime(last_date_et.year, last_date_et.month, last_date_et.day, t.hour, t.minute))
    except Exception:
        return None


def _norm_title(s: str) -> str:
    """
    ì¤‘ë³µ ê¸°ì‚¬ ë³‘í•©ìš© íƒ€ì´í‹€ ì •ê·œí™”
    """
    s = (s or "").strip().lower()
    s = re.sub(r"\s+", " ", s)
    s = re.sub(r"[â€™â€˜Â´`]", "'", s)
    s = re.sub(r"[^a-z0-9ê°€-í£\s'\-:,.!?()/%&]", "", s)
    return s


def fetch_finviz_news_with_links_24h(ticker: str, max_items: int = 120) -> List[Dict[str, str]]:
    """
    Finviz quote í˜ì´ì§€ ë‰´ìŠ¤ í…Œì´ë¸”ì—ì„œ title/url/publishedë¥¼ ìˆ˜ì§‘í•˜ê³ ,
    ìµœê·¼ 24ì‹œê°„(KST ê¸°ì¤€)ë§Œ ë‚¨ê¸´ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜.
    """
    url = f"https://finviz.com/quote.ashx?t={quote(ticker)}"
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 "
            "(KHTML, like Gecko) Chrome/120.0 Safari/537.36"
        )
    }

    r = requests.get(url, headers=headers, timeout=20)
    r.raise_for_status()

    soup = BeautifulSoup(r.text, "lxml")
    table = soup.find("table", class_="news-table")
    if not table:
        return []

    kst = pytz.timezone("Asia/Seoul")
    et = pytz.timezone("US/Eastern")

    now_kst = datetime.datetime.now(kst)
    now_et = now_kst.astimezone(et)
    cutoff_kst = now_kst - datetime.timedelta(hours=24)

    items: List[Dict[str, str]] = []
    last_date_et: Optional[datetime.date] = None

    for row in table.find_all("tr"):
        tds = row.find_all("td")
        if len(tds) < 2:
            continue

        raw_dt = tds[0].get_text(" ", strip=True)  # "Feb-03-26 08:35AM" or "08:12AM"
        a = tds[1].find("a")
        title = a.get_text(" ", strip=True) if a else tds[1].get_text(" ", strip=True)
        link = (a.get("href", "").strip() if a else "")

        if not title:
            continue

        dt_et = _parse_finviz_dt_et(raw_dt, now_et, last_date_et)
        if dt_et is None:
            continue

        last_date_et = dt_et.date()
        dt_kst = dt_et.astimezone(kst)

        if dt_kst < cutoff_kst:
            continue

        items.append(
            {
                "title": title,
                "url": link,
                "published": raw_dt,
                "published_kst": dt_kst.isoformat(),
            }
        )

        if len(items) >= max_items:
            break

    return items


def dedupe_news(items: List[Dict[str, str]]) -> List[Dict[str, str]]:
    """
    ì œëª© ê¸°ë°˜ ì¤‘ë³µ ë³‘í•©
    """
    out: List[Dict[str, str]] = []
    seen = set()
    for it in items:
        key = _norm_title(it.get("title", ""))
        if not key or key in seen:
            continue
        seen.add(key)
        out.append(it)
    return out


# -------------------------------
# Summarization: 10 lines (+ TSLA 20) from Finviz headlines only
# -------------------------------
def summarize_ticker_lines_from_headlines(
    ticker: str,
    company_name: str,
    news_items: List[Dict[str, str]],
    n_lines: int,
    max_headlines_for_llm: int = 12,
) -> str:
    """
    Finvizì—ì„œ ìˆ˜ì§‘í•œ 'í—¤ë“œë¼ì¸ ëª©ë¡'ë§Œìœ¼ë¡œ n_lines ì¤„ í•œê¸€ ìš”ì•½ ìƒì„±.
    (ì›ë¬¸ ë§í¬ëŠ” ì½”ë“œì—ì„œ ë³„ë„ë¡œ ì¶œë ¥)
    """
    if not news_items:
        return "ìµœê·¼ 24ì‹œê°„ ë‚´ Finviz ê¸°ì‚¬ ì—†ìŒ"

    if not OPENAI_API_KEY:
        return "OPENAI_API_KEY ëˆ„ë½"

    use = news_items[:max_headlines_for_llm]

    headline_lines = []
    for i, it in enumerate(use, start=1):
        title = (it.get("title") or "").strip()
        if title:
            headline_lines.append(f"{ticker} N{i}: {title}")
    headlines_text = "\n".join(headline_lines).strip()
    if not headlines_text:
        return "ìµœê·¼ 24ì‹œê°„ ë‚´ Finviz ê¸°ì‚¬ ì—†ìŒ"

    prompt = f"""
Finvizì—ì„œ ìµœê·¼ 24ì‹œê°„ ë‚´ {company_name}({ticker}) ê´€ë ¨ 'í—¤ë“œë¼ì¸ ëª©ë¡'ì´ ì•„ë˜ì— ì£¼ì–´ì§„ë‹¤.
ë„ˆëŠ” ì´ ëª©ë¡ì— ìˆëŠ” ë‚´ìš©ë§Œ ì‚¬ìš©í•´ ìš”ì•½í•´ì•¼ í•œë‹¤.

ê·œì¹™:
- ì•„ë˜ ëª©ë¡ì— ì—†ëŠ” ë‚´ìš©/ë°°ê²½ì§€ì‹/ì¶”ì¸¡/ì¼ë°˜ë¡  ì ˆëŒ€ ê¸ˆì§€
- ì¤‘ë³µ í—¤ë“œë¼ì¸ì€ ê°™ì€ ì‚¬ê±´ì´ë©´ í•˜ë‚˜ë¡œ ë³‘í•©í•˜ì—¬ ìš”ì•½
- ì •í™•íˆ {n_lines}ì¤„ë¡œ í•œê¸€ ìš”ì•½
- ê° ì¤„ì€ ë…ë¦½ì ì¸ í•œ ë¬¸ì¥
- ë²ˆí˜¸/ë¶ˆë¦¿/ì´ëª¨ì§€/ë§ˆí¬ë‹¤ìš´/JSON ê¸ˆì§€ (ì¤„ë°”ê¿ˆë§Œ)
- íšŒì‚¬Â·ì¸ë¬¼Â·ê¸°ê´€ëª…ì€ ê°€ëŠ¥í•œ í•œ ì›ë¬¸ í‘œê¸°ë¥¼ ìœ ì§€í•´ë„ ë¨

[í—¤ë“œë¼ì¸ ëª©ë¡]
{headlines_text}

ìš”ì•½ë§Œ ì¶œë ¥:
""".strip()

    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {OPENAI_API_KEY}"}
    body = {
        "model": OPENAI_MODEL,
        "input": prompt,
        "text": {"format": {"type": "text"}},
    }

    try:
        r = requests.post(OPENAI_URL, headers=headers, json=body, timeout=75)
        if r.status_code != 200:
            return "ìš”ì•½ ìƒì„± ì‹¤íŒ¨"

        j = r.json()
        txt = (_extract_output_text(j) or "").strip()
        if not txt:
            return "ìš”ì•½ ìƒì„± ì‹¤íŒ¨"

        # ì¤„ ìˆ˜ ë³´ì •: ë§ìœ¼ë©´ ìë¥´ê³ , ì ìœ¼ë©´ ê·¸ëŒ€ë¡œ(í™˜ê° ë°©ì§€)
        lines = [ln.strip() for ln in txt.splitlines() if ln.strip()]
        if len(lines) > n_lines:
            lines = lines[:n_lines]
        return "\n".join(lines) if lines else "ìš”ì•½ ìƒì„± ì‹¤íŒ¨"

    except Exception:
        return "ìš”ì•½ ìƒì„± ì‹¤íŒ¨"


# -------------------------------
# Telegram
# -------------------------------
def send_telegram_msg(message: str) -> bool:
    print("Sending Telegram...")
    if not TELEGRAM_TOKEN or not CHAT_ID:
        print("âŒ Telegram env vars missing")
        return False

    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"

    chunks = [message[i : i + TELEGRAM_CHUNK_SIZE] for i in range(0, len(message), TELEGRAM_CHUNK_SIZE)]
    for idx, chunk in enumerate(chunks, start=1):
        payload = {"chat_id": CHAT_ID, "text": chunk, "disable_web_page_preview": False}
        resp = requests.post(url, data=payload, timeout=20)
        if resp.status_code != 200:
            print(f"âŒ Telegram send failed (part {idx}/{len(chunks)}): {resp.text[:400]}")
            return False

    print("âœ… Telegram sent")
    return True


# -------------------------------
# Report builder
# -------------------------------
def build_report_text(today: str) -> str:
    lines: List[str] = []
    lines.append("ğŸ§  [ë¯¸êµ­ì£¼ì‹ ë°ì¼ë¦¬ ë¸Œë¦¬í•‘ (Finviz / ìµœê·¼ 24ì‹œê°„)]")
    lines.append(f"ğŸ“… {today}")
    lines.append("")

    for c in MAG7:
        t = c["ticker"]
        name = c["name"]
        emoji = c["emoji"]

        n_lines = 20 if t == "TSLA" else 10

        try:
            raw = fetch_finviz_news_with_links_24h(t, max_items=120)
            time.sleep(FINVIZ_SLEEP_SEC)
        except Exception as e:
            lines.append(f"{emoji} {t} â€” {name}")
            lines.append("Finviz ìˆ˜ì§‘ ì‹¤íŒ¨")
            lines.append(f"ì—ëŸ¬: {e}")
            lines.append("\n---\n")
            continue

        deduped = dedupe_news(raw)

        summary = summarize_ticker_lines_from_headlines(
            ticker=t,
            company_name=name,
            news_items=deduped,
            n_lines=n_lines,
            max_headlines_for_llm=12,
        )

        lines.append(f"{emoji} {t} â€” {name}")
        lines.append(summary)

        # ì›ë¬¸ ë§í¬: ìƒìœ„ 5ê°œ
        link_items: List[Tuple[str, str]] = []
        for it in deduped[:5]:
            title = (it.get("title") or "").strip()
            url = (it.get("url") or "").strip()
            if title and url:
                link_items.append((title, url))

        if link_items:
            lines.append("")
            lines.append("ì›ë¬¸ ë§í¬")
            for title, url in link_items:
                lines.append(f"- {title}")
                lines.append(f"  {url}")

        lines.append("\n---\n")

    return "\n".join(lines).strip()


# -------------------------------
# Main
# -------------------------------
def main() -> int:
    print("OpenAI key set?", bool(OPENAI_API_KEY))
    print("Token set?", bool(TELEGRAM_TOKEN), "ChatID set?", bool(CHAT_ID))
    print("OpenAI model:", OPENAI_MODEL)

    kst = pytz.timezone("Asia/Seoul")
    today = datetime.datetime.now(kst).strftime("%Y-%m-%d")

    report_text = build_report_text(today)
    ok = send_telegram_msg(report_text)
    return 0 if ok else 1


if __name__ == "__main__":
    sys.exit(main())
